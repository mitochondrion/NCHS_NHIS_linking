{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NCHS linked data (and discard non-NHIS linked data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --recursive ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/datalinkage/linked_mortality\n",
    "!mkdir NCHS_linked_data\n",
    "!mv ./ftp.cdc.gov/pub/Health_Statistics/NCHS/datalinkage/linked_mortality/NHIS_*.dat ./NCHS_linked_data/\n",
    "!rm -r ./ftp.cdc.gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all the NCHS linked mortality datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchs_file_path_pattern = './NCHS_linked_data/NHIS_*.dat'\n",
    "nchs_file_paths = glob.glob('./NCHS_linked_data/NHIS_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert len(nchs_file_paths) > 0, 'No NCHS files available!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchs_column_widths = [14,1,1,3,1,1,1,4,8,8]\n",
    "nchs_column_names = [\"PUBLICID\", \"ELIGSTAT\", \"MORTSTAT\", \"UCOD_LEADING\", \"DIABETES\", \"HYPERTEN\", \"DODQTR\", \"DODYEAR\", \"WGT_NEW\", \"SA_WGT_NEW\"]\n",
    "\n",
    "nchs_dataframes = [\n",
    "    pandas.read_fwf(\n",
    "        file_path,\n",
    "        widths=nchs_column_widths,\n",
    "        names=nchs_column_names,\n",
    "        dtype=False, #{\"PUBLICID\": \"object\"}\n",
    "        na_values=['.']\n",
    "    )\n",
    "    for file_path\n",
    "    in nchs_file_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "total_row_count = sum([dataframe.shape[0] for dataframe in nchs_dataframes])\n",
    "total_line_count_string = !wc -l {nchs_file_path_pattern} | grep total | cut -f2 -d' '\n",
    "total_line_count = int(total_line_count_string[0])\n",
    "\n",
    "failure_message = 'Expected count of {} rows loaded to equal {} lines in linked data files'.format(total_row_count, total_line_count)\n",
    "assert total_row_count == total_line_count, failure_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug info\n",
    "for dataframe in nchs_dataframes:\n",
    "   print(\"{:8d}\".format(dataframe.shape[0]))\n",
    "\n",
    "print(\"=\" * 8)\n",
    "print(\"{:8d}\".format(total_row_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine NCHS dataframes into one big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nchs_data = pandas.concat(nchs_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "separate_row_count = sum([dataframe.shape[0] for dataframe in nchs_dataframes])\n",
    "combined_row_count = nchs_data.shape[0]\n",
    "\n",
    "failure_message = 'Expected count of {} rows in separate dataframes to equal {} combined rows'.format(separate_row_count, combined_row_count)\n",
    "assert separate_row_count == combined_row_count, failure_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the NHIS data extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path of your NHIS data extract (csv)\n",
    "nhis_file_path = '../NHIS/nhis_test.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhis_chunks = pandas.read_csv(\n",
    "    nhis_file_path,\n",
    "    compression='gzip',\n",
    "    chunksize=250000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the NCHS linked data with the raw NHIS data one chunk at a time, discarding unlinked data. Write merged dataframes to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 0 of 0 rows. Processing CHUNK 0...\n",
      "Merged 0 of 250000 rows. Processing CHUNK 1...\n",
      "Merged 0 of 500000 rows. Processing CHUNK 2...\n",
      "Merged 0 of 750000 rows. Processing CHUNK 3...\n",
      "Merged 0 of 1000000 rows. Processing CHUNK 4...\n",
      "Merged 0 of 1250000 rows. Processing CHUNK 5...\n",
      "Merged 0 of 1500000 rows. Processing CHUNK 6...\n",
      "Merged 0 of 1750000 rows. Processing CHUNK 7...\n",
      "Merged 0 of 2000000 rows. Processing CHUNK 8...\n",
      "Merged 0 of 2250000 rows. Processing CHUNK 9...\n",
      "Merged 0 of 2500000 rows. Processing CHUNK 10...\n",
      "Merged 0 of 2750000 rows. Processing CHUNK 11...\n",
      "Merged 0 of 3000000 rows. Processing CHUNK 12...\n",
      "Merged 0 of 3250000 rows. Processing CHUNK 13...\n",
      "Merged 0 of 3500000 rows. Processing CHUNK 14...\n",
      "Merged 0 of 3750000 rows. Processing CHUNK 15...\n",
      "Merged 71826 of 4000000 rows. Processing CHUNK 16...\n",
      "Merged 321826 of 4250000 rows. Processing CHUNK 17...\n",
      "Merged 571826 of 4500000 rows. Processing CHUNK 18...\n",
      "Merged 821826 of 4750000 rows. Processing CHUNK 19...\n",
      "Merged 1071826 of 5000000 rows. Processing CHUNK 20...\n",
      "Merged 1321826 of 5250000 rows. Processing CHUNK 21...\n",
      "Merged 1571826 of 5500000 rows. Processing CHUNK 22...\n",
      "Merged 1710059 of 5750000 rows. Processing CHUNK 23...\n",
      "========\n",
      "Merging complete. Merged 1710059 of 5990154 rows.\n"
     ]
    }
   ],
   "source": [
    "total_rows_processed = 0\n",
    "total_rows_merged = 0\n",
    "\n",
    "for chunk_index, chunk in enumerate(nhis_chunks):\n",
    "    print('Merged {} of {} rows. Processing CHUNK {}...'.format(total_rows_merged, total_rows_processed, chunk_index))\n",
    "\n",
    "    merged_dataframe = pandas.merge(\n",
    "        chunk,\n",
    "        nchs_data,\n",
    "        left_on='NHISPID',\n",
    "        right_on='PUBLICID',\n",
    "        how='inner'#, suffixes=('_ldf', '_rdf')\n",
    "    )\n",
    "\n",
    "    # No need to zip these as they will be discarded later\n",
    "    merged_dataframe.to_csv(\n",
    "        '/tmp/NCHS_NHIS_linked_{}.csv'.format(chunk_index),\n",
    "        index=None,\n",
    "        header=True\n",
    "    )\n",
    "    \n",
    "    total_rows_merged += len(merged_dataframe)\n",
    "    total_rows_processed += len(chunk)\n",
    "\n",
    "print('=' * 8)\n",
    "print('Merging complete. Merged {} of {} rows.'.format(total_rows_merged, total_rows_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "nhis_file_line_count_strings = !gzcat {nhis_file_path} | wc -l\n",
    "nhis_file_line_count = int(nhis_file_line_count_strings[0])\n",
    "\n",
    "failure_message = 'Only processed {} rows out of {} lines of {}'.format(total_rows_processed, nhis_file_line_count, nhis_file_path)\n",
    "assert total_rows_processed == (nhis_file_line_count - 1), failure_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert merged files into one big zipped csv and discard temp csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_file_paths_pattern = '/tmp/NCHS_NHIS_linked_*.csv'\n",
    "merged_data_file_paths = glob.glob(merged_data_file_paths_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "assert len(merged_data_file_paths) > 0, 'No merged files available!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframes = [\n",
    "    pandas.read_csv(file_path)\n",
    "    for file_path\n",
    "    in merged_data_file_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "total_line_count_string = !wc -l {merged_data_file_paths_pattern} | grep total | cut -f2 -d' '\n",
    "total_header_count_string = !ls -1q {merged_data_file_paths_pattern} | wc -l\n",
    "total_line_count = int(total_line_count_string[0]) - int(total_header_count_string[0])\n",
    "total_separate_row_count = sum([dataframe.shape[0] for dataframe in merged_dataframes])\n",
    "\n",
    "failure_message = 'Expected count of {} rows loaded to equal {} lines in merged data files'.format(total_separate_row_count, total_line_count)\n",
    "assert total_line_count == total_separate_row_count, failure_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_merged_dataframe = pandas.concat(merged_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "total_separate_row_count = sum([dataframe.shape[0] for dataframe in merged_dataframes])\n",
    "total_combined_row_count = combined_merged_dataframe.shape[0]\n",
    "\n",
    "failure_message = 'Expected count of {} rows in separate dataframes to equal {} combined rows'.format(total_separate_row_count, total_combined_row_count)\n",
    "assert total_separate_row_count == total_combined_row_count, failure_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_file_path = '/tmp/NCHS_NHIS_linked_data.csv.gz'\n",
    "combined_merged_dataframe.to_csv(\n",
    "    merged_file_path,\n",
    "    index=None,\n",
    "    header=True,\n",
    "    compression='gzip'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "total_combined_row_count = combined_merged_dataframe.shape[0]\n",
    "merged_file_line_count_strings = !gzcat {merged_file_path} | wc -l\n",
    "merged_file_line_count = int(merged_file_line_count_strings[0])\n",
    "\n",
    "failure_message = 'Expected count of {} rows loaded to equal {} lines in {}'.format(merged_file_line_count, total_combined_row_count, merged_file_path)\n",
    "assert total_combined_row_count == (merged_file_line_count - 1), failure_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "!rm /tmp/NCHS_NHIS_linked_*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging of NCHS and NHIS data completed. Output file: /tmp/NCHS_NHIS_linked_data.csv.gz\n"
     ]
    }
   ],
   "source": [
    "print('Merging of NCHS and NHIS data completed. Output file: {}'.format(merged_file_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
